#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Step 5: Complete RAG System with Enhanced Answer Generation
===========================================================

Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ù†Ø¸Ø§Ù… RAG ÙƒØ§Ù…Ù„
- ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø·ÙˆÙ„Ø© ÙˆÙ…ÙØµÙ„Ø© (Ù…Ù‚Ø§Ù„Ø© ÙƒØ§Ù…Ù„Ø©)
- Ù…ØµØ§Ø¯Ø± ÙˆØ§Ø¶Ø­Ø© Ø¨Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (Ù…Ø«Ù„: Ø§Ù„ÙƒØ§ÙÙŠ 1/34)
- Ø¯Ø¹Ù… OpenAI Embeddings
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import yaml
from datetime import datetime

# ChromaDB
import chromadb
from chromadb.config import Settings

# Query Analyzer
from step4_query_analyzer import QueryAnalyzer


# =============================================================================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# =============================================================================

def load_config(config_path: str = "../config.yaml") -> Dict:
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


# =============================================================================
# Embeddings Manager - ÙŠØ¯Ø¹Ù… OpenAI Ùˆ Sentence Transformers
# =============================================================================

class EmbeddingsManager:
    """
    Ù…Ø¯ÙŠØ± Embeddings - ÙŠØ¯Ø¹Ù…:
    1. OpenAI (text-embedding-3-small, text-embedding-3-large)
    2. Sentence Transformers (multilingual-e5-large)
    """

    def __init__(self, provider: str = "openai", model: str = None):
        """
        Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

        Args:
            provider: "openai" Ø£Ùˆ "sentence_transformers"
            model: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        self.provider = provider

        if provider == "openai":
            self._init_openai(model or "text-embedding-3-small")
        else:
            self._init_sentence_transformers(model or "intfloat/multilingual-e5-large")

    def _init_openai(self, model: str):
        """ØªÙ‡ÙŠØ¦Ø© OpenAI"""
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError("ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª openai: pip install openai")

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("ÙŠØ±Ø¬Ù‰ ØªØ¹ÙŠÙŠÙ† OPENAI_API_KEY ÙÙŠ Ù…Ù„Ù .env")

        self.client = OpenAI(api_key=api_key)
        self.model = model

        print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© OpenAI Embeddings: {model}")

    def _init_sentence_transformers(self, model: str):
        """ØªÙ‡ÙŠØ¦Ø© Sentence Transformers"""
        from sentence_transformers import SentenceTransformer

        self.model_obj = SentenceTransformer(model)
        self.model = model

        print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Sentence Transformers: {model}")

    def encode(self, text: str, prefix: str = "query") -> List[float]:
        """
        ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø¥Ù„Ù‰ embedding

        Args:
            text: Ø§Ù„Ù†Øµ
            prefix: Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø© (Ù„Ù„Ù€ E5: "query" Ø£Ùˆ "passage")

        Returns:
            embedding vector
        """
        if self.provider == "openai":
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        else:
            # Sentence Transformers (E5)
            prefixed_text = f"{prefix}: {text}"
            embedding = self.model_obj.encode(
                prefixed_text,
                normalize_embeddings=True,
                convert_to_numpy=True
            )
            return embedding.tolist()


# =============================================================================
# RAG System
# =============================================================================

class RAGSystem:
    """Ù†Ø¸Ø§Ù… RAG ÙƒØ§Ù…Ù„ Ù…Ø¹ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©"""

    def __init__(
        self,
        db_path: str = "../data/database/chroma_db",
        collection_name: str = "islamic_books_e5",
        embeddings_provider: str = "openai",
        embeddings_model: str = None,
        llm_provider: str = "openai",
        llm_model: str = "gpt-4o-mini"
    ):
        """
        Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

        Args:
            db_path: Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            collection_name: Ø§Ø³Ù… collection
            embeddings_provider: "openai" Ø£Ùˆ "sentence_transformers"
            embeddings_model: Ø§Ø³Ù… Ù†Ù…ÙˆØ°Ø¬ embeddings
            llm_provider: "openai" (Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª)
            llm_model: Ù†Ù…ÙˆØ°Ø¬ LLM
        """
        # Query Analyzer
        self.query_analyzer = QueryAnalyzer()

        # Embeddings
        self.embeddings = EmbeddingsManager(
            provider=embeddings_provider,
            model=embeddings_model
        )

        # ChromaDB
        print(f"ğŸ“‚ ÙØªØ­ ChromaDB: {db_path}")
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )

        try:
            self.collection = self.client.get_collection(collection_name)
            print(f"âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ collection: {collection_name}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ collection: {collection_name}")
            raise e

        # LLM Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
        self.llm_provider = llm_provider
        self.llm_model = llm_model

        if llm_provider == "openai":
            try:
                from openai import OpenAI
            except ImportError:
                raise ImportError("ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª openai: pip install openai")

            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("ÙŠØ±Ø¬Ù‰ ØªØ¹ÙŠÙŠÙ† OPENAI_API_KEY ÙÙŠ Ù…Ù„Ù .env")

            self.llm_client = OpenAI(api_key=api_key)
            print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© OpenAI LLM: {llm_model}")

    def search(
        self,
        query: str,
        n_results: int = 10,
        include_types: List[str] = None
    ) -> Dict[str, Any]:
        """
        Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

        Args:
            query: Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            n_results: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            include_types: Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

        Returns:
            Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«
        """
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query_info = self.query_analyzer.analyze(query)

        # ØªÙˆÙ„ÙŠØ¯ embedding
        query_embedding = self.embeddings.encode(query, prefix="query")

        # Ø§Ù„Ø¨Ø­Ø«
        where_filter = None
        if include_types:
            where_filter = {"type": {"$in": include_types}}

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where_filter
        )

        return {
            'query_info': query_info,
            'results': results
        }

    def build_sources_list(self, metadatas: List[Dict]) -> str:
        """
        Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©

        Args:
            metadatas: metadata Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«

        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ù†Ø³Ù‚Ø©
        """
        sources_text = "### Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©:\n\n"

        for i, meta in enumerate(metadatas, 1):
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±
            source_type = meta.get('type', '')

            if source_type == 'paragraph':
                book = meta.get('parent_doc', '').split('_vol')[0] if meta.get('parent_doc') else 'ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
                page = meta.get('page', '')
                source_name = f"{book} (Øµ {page})" if page else book

            elif source_type == 'section':
                parent = meta.get('parent_doc', '')
                book = parent.split('_vol')[0] if parent else 'ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
                title = meta.get('title', '')
                source_name = f"{book} - {title}" if title else book

            elif source_type == 'document':
                book = meta.get('book', 'ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
                volume = meta.get('volume', '')
                source_name = f"{book} ({volume})" if volume else book

            else:
                source_name = "Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

            sources_text += f"{i}. {source_name}\n"

        return sources_text

    def build_context(self, documents: List[str], metadatas: List[Dict]) -> str:
        """
        Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬

        Args:
            documents: Ù†ØµÙˆØµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            metadatas: metadata Ø§Ù„Ù†ØªØ§Ø¦Ø¬

        Returns:
            Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù†Ø³Ù‚
        """
        context = "### Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©:\n\n"

        for i, (doc, meta) in enumerate(zip(documents, metadatas), 1):
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±
            source_type = meta.get('type', '')

            if source_type == 'paragraph':
                book = meta.get('parent_doc', '').split('_vol')[0] if meta.get('parent_doc') else 'ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
                page = meta.get('page', '')
                source_info = f"{book} (Øµ {page})" if page else book

            elif source_type == 'section':
                parent = meta.get('parent_doc', '')
                book = parent.split('_vol')[0] if parent else 'ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
                title = meta.get('title', '')
                source_info = f"{book} - {title}" if title else book

            elif source_type == 'document':
                book = meta.get('book', 'ÙƒØªØ§Ø¨ ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
                volume = meta.get('volume', '')
                source_info = f"{book} ({volume})" if volume else book

            else:
                source_info = "Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

            context += f"**[{i}] Ù…Ù† {source_info}:**\n{doc}\n\n"

        return context

    def generate_answer(
        self,
        query: str,
        context: str,
        sources_list: str,
        query_info: Dict
    ) -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙØµÙ„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM

        Args:
            query: Ø§Ù„Ø³Ø¤Ø§Ù„
            context: Ø§Ù„Ø³ÙŠØ§Ù‚
            sources_list: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±
            query_info: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„

        Returns:
            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
        """
        # ØªØ­Ø¯ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        requires_detailed = query_info.get('requires_detailed_answer', True)

        # Ø¨Ù†Ø§Ø¡ prompt Ù…Ø­Ø³Ù‘Ù†
        system_prompt = """Ø£Ù†Øª Ø¹Ø§Ù„Ù… Ø¯ÙŠÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù„Ù…ÙŠØ© ÙˆØ¯Ù‚ÙŠÙ‚Ø©.

**Ù…Ù‡Ù…ØªÙƒ:**
1. Ø§ÙƒØªØ¨ Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙØµÙ„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ (Ù…Ø«Ù„ Ù…Ù‚Ø§Ù„Ø© Ø¹Ù„Ù…ÙŠØ© ÙƒØ§Ù…Ù„Ø©)
2. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø© Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
3. Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØµØ­ÙŠØ­ Ù…Ø«Ù„: "Ø§Ù„ÙƒØ§ÙÙŠ (1/34)" Ø£Ùˆ "Ù†Ù‡Ø¬ Ø§Ù„Ø¨Ù„Ø§ØºØ© (Øµ 156)"

**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:**
âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±
âœ… Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ù…Ø«Ù„: "ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„ÙƒØ§ÙÙŠ (1/34) Ø£Ù†..."
âœ… Ø§ÙƒØªØ¨ Ø¥Ø¬Ø§Ø¨Ø© Ø·ÙˆÙŠÙ„Ø© ÙˆÙ…ÙØµÙ„Ø© (Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 5-10 ÙÙ‚Ø±Ø§Øª)
âœ… Ù‚Ø³Ù‘Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ø¶Ø­Ø©
âœ… ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ Ø£Ø¶Ù Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©

âŒ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… [Ø§Ù„Ù…ØµØ¯Ø± 1] Ø£Ùˆ [Ø§Ù„Ù…ØµØ¯Ø± 2]
âŒ Ù„Ø§ ØªÙƒØªØ¨ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù‚ØµÙŠØ±Ø©
âŒ Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ

**Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
- Ù…Ù‚Ø¯Ù…Ø©
- Ø´Ø±Ø­ ØªÙØµÙŠÙ„ÙŠ (Ø¹Ø¯Ø© ÙÙ‚Ø±Ø§Øª)
- Ø£Ù…Ø«Ù„Ø© ÙˆØ´ÙˆØ§Ù‡Ø¯ Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±
- Ø®Ø§ØªÙ…Ø©
- Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""

        user_prompt = f"""
{sources_list}

{context}

---

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {query}

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:** Ø§ÙƒØªØ¨ Ù…Ù‚Ø§Ù„Ø© Ø¹Ù„Ù…ÙŠØ© Ø´Ø§Ù…Ù„Ø© ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø£Ø³Ù…Ø§Ø¦Ù‡Ø§ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© (Ù…Ø«Ù„: Ø§Ù„ÙƒØ§ÙÙŠ 1/34ØŒ Ù†Ù‡Ø¬ Ø§Ù„Ø¨Ù„Ø§ØºØ© Øµ 156).

Ø§ÙƒØªØ¨ Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙØµÙ„Ø© Ù„Ø§ ØªÙ‚Ù„ Ø¹Ù† 5 ÙÙ‚Ø±Ø§ØªØŒ ÙˆØ§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØµØ­ÙŠØ­.
"""

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LLM
        if self.llm_provider == "openai":
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=3000  # Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø·ÙˆÙŠÙ„Ø©
            )

            return response.choices[0].message.content

        return "Ø®Ø·Ø£: LLM provider ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"

    def query(
        self,
        question: str,
        n_results: int = 10,
        include_types: List[str] = None
    ) -> Dict[str, Any]:
        """
        Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©

        Args:
            question: Ø§Ù„Ø³Ø¤Ø§Ù„
            n_results: Ø¹Ø¯Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«
            include_types: Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

        Returns:
            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±
        """
        print(f"\nğŸ” Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n")

        # 1. Ø§Ù„Ø¨Ø­Ø«
        print("ğŸ“Š Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        search_results = self.search(question, n_results, include_types)

        query_info = search_results['query_info']
        results = search_results['results']

        documents = results['documents'][0]
        metadatas = results['metadatas'][0]

        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(documents)} Ù†ØªÙŠØ¬Ø©\n")

        # 2. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±
        print("ğŸ“š Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±...")
        sources_list = self.build_sources_list(metadatas)

        # 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚
        print("ğŸ“ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚...")
        context = self.build_context(documents, metadatas)

        # 4. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        print("ğŸ¤– ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...\n")
        answer = self.generate_answer(question, context, sources_list, query_info)

        return {
            'question': question,
            'answer': answer,
            'query_info': query_info,
            'num_sources': len(documents),
            'timestamp': datetime.now().isoformat()
        }


# =============================================================================
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© - Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
# =============================================================================

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…"""

    print("=" * 70)
    print("ğŸš€ Multi-Level RAG System - Enhanced Version")
    print("=" * 70)
    print()

    # ØªØ­Ù…ÙŠÙ„ Ù…Ù† .env Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass

    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
    print("âš™ï¸ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG...\n")

    # ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± provider Ù‡Ù†Ø§:
    # - embeddings_provider="openai" Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI
    # - embeddings_provider="sentence_transformers" Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… E5

    rag = RAGSystem(
        embeddings_provider="openai",  # Ø£Ùˆ "sentence_transformers"
        embeddings_model="text-embedding-3-small",  # Ø£Ùˆ "intfloat/multilingual-e5-large"
        llm_provider="openai",
        llm_model="gpt-4o-mini"
    )

    # Ø§Ø®ØªØ¨Ø§Ø±
    test_question = "Ù…Ø§ Ù‡Ùˆ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¥Ù…Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø°Ù‡Ø¨ Ø§Ù„Ø´ÙŠØ¹ÙŠØŸ"

    result = rag.query(test_question, n_results=10)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
    print("=" * 70)
    print("ğŸ“ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
    print("=" * 70)
    print()
    print(result['answer'])
    print()
    print("=" * 70)
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… {result['num_sources']} Ù…ØµØ¯Ø±")
    print("=" * 70)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Step 3: Generate Embeddings and Build Vector Database (E5 Version)
===================================================================

Ø§Ù„Ù…Ù‡Ù…Ø©:
- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Step 2
- ØªÙˆÙ„ÙŠØ¯ embeddings Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… multilingual-e5-large (Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©)
- Ø¨Ù†Ø§Ø¡ ChromaDB vector database
- Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«

Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: intfloat/multilingual-e5-large
- Ø§Ù„Ø­Ø¬Ù…: 560M parameters
- Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯: 1024 (Ø£ÙƒØ¨Ø± Ù…Ù† paraphrase)
- Ø§Ù„Ù„ØºØ§Øª: 100 Ù„ØºØ©
- Ø§Ù„Ø¬ÙˆØ¯Ø©: state-of-the-art Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
"""

import os
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import yaml


# =============================================================================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# =============================================================================

def load_config(config_path: str = "config.yaml") -> Dict:
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


config = load_config()


# =============================================================================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† config.yaml
# =============================================================================

# Paths
PROCESSED_DIR = Path(config['paths']['processed_data'])
DATABASE_DIR = Path(config['paths']['database'])
CHROMA_DB_PATH = Path(config['paths']['chroma_db'])

# Embeddings - E5 Model
MODEL_NAME = config['embeddings']['model']  # intfloat/multilingual-e5-large
EMBEDDING_DIM = config['embeddings']['dimension']  # 1024
BATCH_SIZE = config['embeddings']['batch_size']
DEVICE = config['embeddings']['device']
SHOW_PROGRESS = config['embeddings']['show_progress']
CACHE_FOLDER = Path(config['embeddings']['cache_folder'])

# Files
DOCUMENTS_FILE = PROCESSED_DIR / "documents.json"
SECTIONS_FILE = PROCESSED_DIR / "sections.json"
PARAGRAPHS_FILE = PROCESSED_DIR / "paragraphs.json"
STATS_FILE = DATABASE_DIR / "embeddings_stats.json"


# =============================================================================
# Class: EmbeddingsGenerator (E5 Version)
# =============================================================================

class EmbeddingsGenerator:
    """
    Ù…ÙˆÙ„Ø¯ Embeddings Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… multilingual-e5-large
    
    Ù…Ù„Ø§Ø­Ø¸Ø©: E5 ÙŠØ­ØªØ§Ø¬ prefix Ù„Ù„Ù†ØµÙˆØµ:
    - "query: " Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    - "passage: " Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®Ø²Ù†Ø©
    """
    
    def __init__(self, model_name: str = MODEL_NAME, device: str = DEVICE):
        """
        Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        
        Args:
            model_name: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (intfloat/multilingual-e5-large)
            device: Ø§Ù„Ø¬Ù‡Ø§Ø² (cpu Ø£Ùˆ cuda)
        """
        print(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings: {model_name}")
        print(f"âš™ï¸ Ø§Ù„Ø¬Ù‡Ø§Ø²: {device}")
        print(f"â„¹ï¸ Ù‡Ø°Ø§ Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© - Microsoft E5")
        print(f"â„¹ï¸ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯: 1024 (Ø£ÙƒØ¨Ø± Ù…Ù† paraphrase-768)")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ cache
        CACHE_FOLDER.mkdir(parents=True, exist_ok=True)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        start_time = time.time()
        self.model = SentenceTransformer(
            model_name,
            device=device,
            cache_folder=str(CACHE_FOLDER)
        )
        load_time = time.time() - start_time
        
        print(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ {load_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ“Š Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ÙØ¹Ù„ÙŠØ©: {self.model.get_sentence_embedding_dimension()}")
        print()
    
    def encode_batch(
        self, 
        texts: List[str], 
        show_progress: bool = True,
        prefix: str = "passage"
    ) -> List[List[float]]:
        """
        ØªØ­ÙˆÙŠÙ„ Ù†ØµÙˆØµ Ø¥Ù„Ù‰ embeddings
        
        Args:
            texts: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ
            show_progress: Ø¥Ø¸Ù‡Ø§Ø± progress bar
            prefix: Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø© ("passage" Ø£Ùˆ "query")
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© embeddings
        """
        # Ø¥Ø¶Ø§ÙØ© prefix Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Øµ
        # Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù†Ø³ØªØ®Ø¯Ù… "passage: "
        prefixed_texts = [f"{prefix}: {text}" for text in texts]
        
        embeddings = self.model.encode(
            prefixed_texts,
            batch_size=BATCH_SIZE,
            show_progress_bar=show_progress,
            convert_to_numpy=True,
            normalize_embeddings=True  # L2 normalization
        )
        
        return embeddings.tolist()


# =============================================================================
# Class: ChromaDBBuilder
# =============================================================================

class ChromaDBBuilder:
    """Ø¨Ù†Ø§Ø¡ ChromaDB vector database"""
    
    def __init__(self, db_path: Path = CHROMA_DB_PATH):
        """
        Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        
        Args:
            db_path: Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        self.db_path = db_path
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯
        db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡/ÙØªØ­ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        print(f"ğŸ“‚ ÙØªØ­ ChromaDB: {db_path}")
        self.client = chromadb.PersistentClient(
            path=str(db_path),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # Collection name
        self.collection_name = "islamic_books_e5"
        
        # Ø­Ø°Ù collection Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ù† ÙˆØ¬Ø¯
        try:
            self.client.delete_collection(self.collection_name)
            print(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù collection Ø§Ù„Ù‚Ø¯ÙŠÙ…")
        except:
            pass
        
        # Ø¥Ù†Ø´Ø§Ø¡ collection Ø¬Ø¯ÙŠØ¯
        self.collection = self.client.create_collection(
            name=self.collection_name,
            metadata={
                "description": "Multi-level RAG for Islamic books using E5",
                "model": "intfloat/multilingual-e5-large",
                "dimension": 1024
            }
        )
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ collection: {self.collection_name}")
    
    def add_items(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]]
    ):
        """
        Ø¥Ø¶Ø§ÙØ© Ø¹Ù†Ø§ØµØ± Ø¥Ù„Ù‰ ChromaDB
        
        Args:
            ids: Ù…Ø¹Ø±ÙØ§Øª ÙØ±ÙŠØ¯Ø©
            embeddings: embeddings vectors
            documents: Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ØµÙ„ÙŠØ©
            metadatas: metadata Ù„ÙƒÙ„ Ø¹Ù†ØµØ±
        """
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
    
    def get_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        count = self.collection.count()
        
        # Ø¹Ø¯ ÙƒÙ„ Ù†ÙˆØ¹
        all_data = self.collection.get()
        docs_count = len([m for m in all_data['metadatas'] if m.get('type') == 'document'])
        secs_count = len([m for m in all_data['metadatas'] if m.get('type') == 'section'])
        paras_count = len([m for m in all_data['metadatas'] if m.get('type') == 'paragraph'])
        
        return {
            "total_items": count,
            "documents": docs_count,
            "sections": secs_count,
            "paragraphs": paras_count
        }


# =============================================================================
# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
# =============================================================================

def load_json(file_path: Path) -> List[Dict]:
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON"""
    print(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„: {file_path.name}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(data)} Ø¹Ù†ØµØ±")
    return data


def prepare_items(items: List[Dict], item_type: str) -> tuple:
    """
    ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù„Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ ChromaDB
    
    Args:
        items: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ±
        item_type: Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù†ØµØ± (document, section, paragraph)
    
    Returns:
        (ids, texts, metadatas)
    """
    ids = []
    texts = []
    metadatas = []
    
    for item in items:
        # ID
        if item_type == 'document':
            item_id = item['doc_id']
        elif item_type == 'section':
            item_id = item['section_id']
        else:  # paragraph
            item_id = item['para_id']
        
        ids.append(item_id)
        
        # Text - Ø§Ø³ØªØ®Ø¯Ø§Ù… summary Ù„Ù„Ù€ documents
        if item_type == 'document':
            texts.append(item.get('summary', item.get('text', '')))
        else:
            texts.append(item.get('text', ''))
        
        # Metadata
        metadata = {
            'type': item_type,
            'word_count': item.get('stats', {}).get('word_count', 0)
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        if item_type == 'document':
            metadata.update({
                'book': item.get('book', ''),
                'volume': item.get('volume', 0),
                'author': item.get('author', '')
            })
        elif item_type == 'section':
            metadata.update({
                'parent_doc': item.get('parent_doc', ''),
                'title': item.get('title', ''),
                'main_topic': item.get('main_topic', '')
            })
        else:  # paragraph
            metadata.update({
                'parent_section': item.get('parent_section', ''),
                'parent_doc': item.get('parent_doc', ''),
                'page': str(item.get('stats', {}).get('page', ''))
            })
        
        metadatas.append(metadata)
    
    return ids, texts, metadatas


# =============================================================================
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# =============================================================================

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    print("=" * 70)
    print("ğŸš€ Step 3: Embeddings with multilingual-e5-large")
    print("=" * 70)
    print("â„¹ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© - Microsoft E5")
    print("â„¹ï¸ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯: 1024 (Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 768)")
    print()
    
    start_time = time.time()
    
    # =============================================================================
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("-" * 70)
    
    documents = load_json(DOCUMENTS_FILE)
    sections = load_json(SECTIONS_FILE)
    paragraphs = load_json(PARAGRAPHS_FILE)
    
    total_items = len(documents) + len(sections) + len(paragraphs)
    print(f"\nğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_items} Ø¹Ù†ØµØ±")
    print()
    
    # =============================================================================
    # 2. ØªÙ‡ÙŠØ¦Ø© Embeddings Generator
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙ‡ÙŠØ¦Ø© E5 Embeddings Generator")
    print("-" * 70)
    
    generator = EmbeddingsGenerator()
    
    # =============================================================================
    # 3. ØªÙ‡ÙŠØ¦Ø© ChromaDB
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙ‡ÙŠØ¦Ø© ChromaDB")
    print("-" * 70)
    
    db = ChromaDBBuilder()
    print()
    
    # =============================================================================
    # 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Documents
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ù…Ø¹Ø§Ù„Ø¬Ø© Documents")
    print("-" * 70)
    
    doc_ids, doc_texts, doc_metadatas = prepare_items(documents, 'document')
    
    print(f"ğŸ”¢ ØªÙˆÙ„ÙŠØ¯ E5 embeddings Ù„Ù€ {len(doc_texts)} document...")
    doc_embeddings = generator.encode_batch(
        doc_texts, 
        show_progress=SHOW_PROGRESS,
        prefix="passage"
    )
    
    print(f"ğŸ’¾ Ø¥Ø¶Ø§ÙØ© Documents Ø¥Ù„Ù‰ ChromaDB...")
    db.add_items(doc_ids, doc_embeddings, doc_texts, doc_metadatas)
    print("âœ… ØªÙ…")
    print()
    
    # =============================================================================
    # 5. Ù…Ø¹Ø§Ù„Ø¬Ø© Sections
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ù…Ø¹Ø§Ù„Ø¬Ø© Sections")
    print("-" * 70)
    
    sec_ids, sec_texts, sec_metadatas = prepare_items(sections, 'section')
    
    print(f"ğŸ”¢ ØªÙˆÙ„ÙŠØ¯ E5 embeddings Ù„Ù€ {len(sec_texts)} section...")
    sec_embeddings = generator.encode_batch(
        sec_texts, 
        show_progress=SHOW_PROGRESS,
        prefix="passage"
    )
    
    print(f"ğŸ’¾ Ø¥Ø¶Ø§ÙØ© Sections Ø¥Ù„Ù‰ ChromaDB...")
    db.add_items(sec_ids, sec_embeddings, sec_texts, sec_metadatas)
    print("âœ… ØªÙ…")
    print()
    
    # =============================================================================
    # 6. Ù…Ø¹Ø§Ù„Ø¬Ø© Paragraphs
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ù…Ø¹Ø§Ù„Ø¬Ø© Paragraphs")
    print("-" * 70)
    
    para_ids, para_texts, para_metadatas = prepare_items(paragraphs, 'paragraph')
    
    print(f"ğŸ”¢ ØªÙˆÙ„ÙŠØ¯ E5 embeddings Ù„Ù€ {len(para_texts)} paragraph...")
    para_embeddings = generator.encode_batch(
        para_texts, 
        show_progress=SHOW_PROGRESS,
        prefix="passage"
    )
    
    print(f"ğŸ’¾ Ø¥Ø¶Ø§ÙØ© Paragraphs Ø¥Ù„Ù‰ ChromaDB...")
    db.add_items(para_ids, para_embeddings, para_texts, para_metadatas)
    print("âœ… ØªÙ…")
    print()
    
    # =============================================================================
    # 7. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
    print("-" * 70)
    
    db_stats = db.get_stats()
    
    total_time = time.time() - start_time
    
    stats = {
        "timestamp": datetime.now().isoformat(),
        "model": MODEL_NAME,
        "model_type": "Microsoft E5 (state-of-the-art for Arabic)",
        "embedding_dimension": EMBEDDING_DIM,
        "device": DEVICE,
        
        "data": {
            "documents": len(documents),
            "sections": len(sections),
            "paragraphs": len(paragraphs),
            "total": total_items
        },
        
        "database": db_stats,
        
        "performance": {
            "total_time_seconds": round(total_time, 2),
            "total_time_minutes": round(total_time / 60, 2),
            "items_per_second": round(total_items / total_time, 2)
        },
        
        "model_info": {
            "advantages": [
                "Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©",
                "1024 Ø£Ø¨Ø¹Ø§Ø¯ (Ø£ÙƒØ¨Ø± Ù…Ù† paraphrase-768)",
                "100 Ù„ØºØ©",
                "state-of-the-art performance"
            ]
        }
    }
    
    # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    DATABASE_DIR.mkdir(parents=True, exist_ok=True)
    with open(STATS_FILE, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    print(f"ğŸ“Š Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:")
    print(f"   - Documents: {stats['data']['documents']}")
    print(f"   - Sections: {stats['data']['sections']}")
    print(f"   - Paragraphs: {stats['data']['paragraphs']}")
    print(f"   - Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {stats['data']['total']}")
    print()
    print(f"ğŸ’¾ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    print(f"   - Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙÙŠ ChromaDB: {db_stats['total_items']}")
    print(f"   - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}")
    print(f"   - Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯: {EMBEDDING_DIM}")
    print(f"   - Ø§Ù„Ù…Ø³Ø§Ø±: {CHROMA_DB_PATH}")
    print()
    print(f"â±ï¸ Ø§Ù„Ø£Ø¯Ø§Ø¡:")
    print(f"   - Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {stats['performance']['total_time_minutes']:.2f} Ø¯Ù‚ÙŠÙ‚Ø©")
    print(f"   - Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['performance']['items_per_second']:.2f} Ø¹Ù†ØµØ±/Ø«Ø§Ù†ÙŠØ©")
    print()
    print(f"ğŸ’¾ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {STATS_FILE}")
    print()
    
    # =============================================================================
    # 8. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«
    # =============================================================================
    
    print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 8: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø« (E5)")
    print("-" * 70)
    
    # ØªØ¬Ø±Ø¨Ø© Ø¨Ø­Ø« Ø¨Ø³ÙŠØ·Ø©
    test_query = "Ø§Ù„Ø¥Ù…Ø§Ù…Ø©"
    print(f"ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: '{test_query}'")
    print(f"â„¹ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: E5 ÙŠØ­ØªØ§Ø¬ prefix 'query:' Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª")
    
    # Ù…Ù‡Ù…: Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù†Ø³ØªØ®Ø¯Ù… prefix "query: "
    query_embedding = generator.encode_batch(
        [test_query], 
        show_progress=False,
        prefix="query"  # Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    )[0]
    
    results = db.collection.query(
        query_embeddings=[query_embedding],
        n_results=3
    )
    
    print(f"\nğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ø£ÙˆÙ„ 3):")
    for i, (doc_id, metadata) in enumerate(zip(results['ids'][0], results['metadatas'][0]), 1):
        print(f"\n{i}. ID: {doc_id}")
        print(f"   Ø§Ù„Ù†ÙˆØ¹: {metadata['type']}")
        if metadata['type'] == 'section':
            print(f"   Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {metadata['title']}")
        elif metadata['type'] == 'document':
            print(f"   Ø§Ù„ÙƒØªØ§Ø¨: {metadata['book']}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {metadata['word_count']}")
    
    print()
    
    # =============================================================================
    # Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    # =============================================================================
    
    print("=" * 70)
    print("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Step 3 Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… E5!")
    print("=" * 70)
    print()
    print("ğŸ‰ Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙƒ Ø£Ù‚ÙˆÙ‰ embeddings Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©!")
    print(f"   - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}")
    print(f"   - Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯: {EMBEDDING_DIM}")
    print(f"   - Ø§Ù„Ø¬ÙˆØ¯Ø©: state-of-the-art")
    print()
    print("ğŸ“¦ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©:")
    print(f"   - {CHROMA_DB_PATH}")
    print(f"   - {STATS_FILE}")
    print()
    print("ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: Step 4 - Query Analyzer")
    print()


# =============================================================================
# Ø§Ù„ØªØ´ØºÙŠÙ„
# =============================================================================

if __name__ == "__main__":
    main()
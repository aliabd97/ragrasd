#!/usr/bin/env python3
"""
Multi-Level AI Chunking with Citation Extraction

Ù†Ø¸Ø§Ù… ØªÙ‚Ø³ÙŠÙ… Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Claude API:
- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ§Ø¯Ø±
- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… (sections)
- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª (paragraphs)

Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ø§Ù„Ø­ÙØ§Ø¸ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ§Ø¯Ø± 100%
"""

import os
import sys
import json
import yaml
import re
from pathlib import Path
from typing import Dict, List, Any
from tqdm import tqdm
from dotenv import load_dotenv

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ù€ imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.validators import (
    validate_citations,
    validate_sections_citations,
    validate_paragraphs_citations,
    validate_text_preservation
)

try:
    from anthropic import Anthropic
except ImportError:
    print("âŒ Error: anthropic library not installed")
    print("   Run: pip install anthropic")
    sys.exit(1)


def load_config() -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† config.yaml"""
    config_path = Path(__file__).parent.parent / 'config.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_text_file(filepath: str) -> str:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ù†ØµÙŠ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return f.read()


def save_json(data: Any, filepath: str) -> None:
    """Ø­ÙØ¸ JSON"""
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_json(filepath: str) -> Any:
    """ØªØ­Ù…ÙŠÙ„ JSON"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def parse_json_response(text: str) -> Dict[str, Any]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ù…Ù† Ø±Ø¯ Claude

    ÙŠØ²ÙŠÙ„ markdown code blocks ÙˆÙŠØ­Ù„Ù„ JSON
    """
    # Ø¥Ø²Ø§Ù„Ø© markdown code blocks
    text = re.sub(r'```json\s*\n?', '', text)
    text = re.sub(r'```\s*\n?', '', text)
    text = text.strip()

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        print(f"âŒ Error parsing JSON: {e}")
        print(f"Response text (first 500 chars):\n{text[:500]}")
        raise


class AIChunker:
    """
    AI-powered chunking system

    ÙŠØ³ØªØ®Ø¯Ù… Claude API Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø°ÙƒØ§Ø¡
    """

    def __init__(self, config: Dict[str, Any]):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…

        Args:
            config: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† config.yaml
        """
        # ØªØ­Ù…ÙŠÙ„ .env
        load_dotenv()

        # Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        self.config = config.get('chunking', {
            'model': 'claude-sonnet-4-20250514',
            'max_tokens': 16000,
            'temperature': 0,
            'api_key_env': 'ANTHROPIC_API_KEY'
        })

        # Claude client
        api_key = os.getenv(self.config['api_key_env'])
        if not api_key:
            raise ValueError(f"Missing API key: {self.config['api_key_env']}")

        self.client = Anthropic(api_key=api_key)

        # ØªØ­Ù…ÙŠÙ„ prompts
        self.load_prompts()

        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'total_cost': 0.0,
            'total_input_tokens': 0,
            'total_output_tokens': 0,
            'volumes_processed': 0,
            'api_calls': 0
        }

    def load_prompts(self) -> None:
        """ØªØ­Ù…ÙŠÙ„ prompt templates"""
        prompts_dir = Path(__file__).parent / 'prompts'

        # Extract citations prompt
        with open(prompts_dir / 'extract_citations.txt', 'r', encoding='utf-8') as f:
            self.extract_citations_prompt = f.read()

        # Create sections prompt
        with open(prompts_dir / 'create_sections.txt', 'r', encoding='utf-8') as f:
            self.create_sections_prompt = f.read()

        # Create paragraphs prompt
        with open(prompts_dir / 'create_paragraphs.txt', 'r', encoding='utf-8') as f:
            self.create_paragraphs_prompt = f.read()

        print("âœ… Loaded prompt templates")

    def call_claude(self, prompt: str, desc: str = "Processing") -> str:
        """
        Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Claude API

        Args:
            prompt: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø±Ø³Ø§Ù„Ù‡
            desc: ÙˆØµÙ Ù„Ù„Ø¹Ù…Ù„ÙŠØ©

        Returns:
            str: Ø±Ø¯ Claude
        """
        print(f"   ğŸ¤– Calling Claude API: {desc}...")

        response = self.client.messages.create(
            model=self.config['model'],
            max_tokens=self.config['max_tokens'],
            temperature=self.config['temperature'],
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats['api_calls'] += 1
        self.stats['total_input_tokens'] += response.usage.input_tokens
        self.stats['total_output_tokens'] += response.usage.output_tokens

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒÙ„ÙØ© (Claude Sonnet 4 pricing)
        # Input: $3 per million tokens
        # Output: $15 per million tokens
        input_cost = response.usage.input_tokens * 3 / 1_000_000
        output_cost = response.usage.output_tokens * 15 / 1_000_000
        call_cost = input_cost + output_cost
        self.stats['total_cost'] += call_cost

        print(f"      Tokens: {response.usage.input_tokens} in, {response.usage.output_tokens} out")
        print(f"      Cost: ${call_cost:.4f}")

        return response.content[0].text

    def extract_citations(self, volume_num: int, text: str) -> Dict[str, Any]:
        """
        Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ§Ø¯Ø±

        Args:
            volume_num: Ø±Ù‚Ù… Ø§Ù„Ø¬Ø²Ø¡
            text: Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„

        Returns:
            Dict: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ø±
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“š Phase 1: Extracting citations from volume {volume_num}")
        print(f"{'='*60}")

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
        prompt = self.extract_citations_prompt.format(
            volume=volume_num,
            full_text=text
        )

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Claude
        response = self.call_claude(prompt, f"Extract citations v{volume_num}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¯
        citations_data = parse_json_response(response)

        # Ø§Ù„ØªØ­Ù‚Ù‚
        try:
            validate_citations(citations_data)
        except AssertionError as e:
            print(f"âš ï¸  Validation warning: {e}")

        # Ø­ÙØ¸
        output_path = f"data/processed/citations_extracted/citations_Ø¬{volume_num}.json"
        save_json(citations_data, output_path)

        print(f"âœ… Extracted {len(citations_data.get('citations', []))} citations")
        print(f"   Saved to: {output_path}")

        return citations_data

    def create_sections(self, volume_num: int, text: str,
                       citations_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ sections

        Args:
            volume_num: Ø±Ù‚Ù… Ø§Ù„Ø¬Ø²Ø¡
            text: Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„
            citations_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù…Ù† Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1

        Returns:
            Dict: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“‘ Phase 2: Creating sections for volume {volume_num}")
        print(f"{'='*60}")

        # Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ø¹ÙŠÙ†Ø©)
        citations_map = [
            {
                'id': c['citation_id'],
                'appearance': c.get('appearance_in_text', ''),
                'book': c['source']['book_name'],
                'reference': c['source']['full_reference']
            }
            for c in citations_data['citations'][:50]  # Ø£ÙˆÙ„ 50 Ù„Ù„ØªÙˆØ¶ÙŠØ­
        ]

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
        prompt = self.create_sections_prompt.format(
            volume=volume_num,
            total_citations=len(citations_data['citations']),
            citations_map=json.dumps(citations_map, ensure_ascii=False, indent=2),
            full_text=text
        )

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Claude
        response = self.call_claude(prompt, f"Create sections v{volume_num}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¯
        sections_data = parse_json_response(response)

        # Ø§Ù„ØªØ­Ù‚Ù‚
        try:
            validate_sections_citations(sections_data, citations_data)
        except Exception as e:
            print(f"âš ï¸  Validation warning: {e}")

        print(f"âœ… Created {len(sections_data.get('sections', []))} sections")

        return sections_data

    def create_paragraphs(self, section: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙ‚Ø³ÙŠÙ… section Ø¥Ù„Ù‰ paragraphs

        Args:
            section: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø³Ù…

        Returns:
            List[Dict]: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙÙ‚Ø±Ø§Øª
        """
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt
        prompt = self.create_paragraphs_prompt.format(
            section_id=section['section_id'],
            section_title=section.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†'),
            word_count=section.get('word_count', 0),
            citations_count=len(section.get('citations', [])),
            section_citations=json.dumps(
                section.get('citations', []),
                ensure_ascii=False,
                indent=2
            ),
            section_text=section.get('text', '')
        )

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Claude
        response = self.call_claude(prompt, f"Create paragraphs")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¯
        paragraphs_data = parse_json_response(response)

        # Ø§Ù„ØªØ­Ù‚Ù‚
        try:
            validate_paragraphs_citations(
                paragraphs_data,
                section.get('citations', [])
            )
        except Exception as e:
            print(f"âš ï¸  Validation warning: {e}")

        return paragraphs_data.get('paragraphs', [])

    def process_volume(self, volume_num: int, filepath: str) -> Dict[str, Any]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø²Ø¡ ÙƒØ§Ù…Ù„

        Args:
            volume_num: Ø±Ù‚Ù… Ø§Ù„Ø¬Ø²Ø¡
            filepath: Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù†Øµ

        Returns:
            Dict: Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“– Processing Volume {volume_num}")
        print(f"{'='*60}")

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ
        text = load_text_file(filepath)
        word_count = len(text.split())
        print(f"ğŸ“„ Loaded {word_count:,} words from {filepath}")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ§Ø¯Ø±
        citations_data = self.extract_citations(volume_num, text)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
        sections_data = self.create_sections(volume_num, text, citations_data)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ù„ÙƒÙ„ Ù‚Ø³Ù…
        print(f"\n{'='*60}")
        print(f"ğŸ“ Phase 3: Creating paragraphs")
        print(f"{'='*60}")

        all_paragraphs = []
        sections = sections_data.get('sections', [])

        for section in tqdm(sections, desc="Processing sections"):
            # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª
            paragraphs = self.create_paragraphs(section)

            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¨
            for para in paragraphs:
                para['parent_section'] = section['section_id']
                para['parent_doc'] = f"shafi_v{volume_num}"

            all_paragraphs.extend(paragraphs)

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø³Ù… Ø¨Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙÙ‚Ø±Ø§Øª
            section['children_paragraphs'] = [p['para_id'] for p in paragraphs]

        print(f"âœ… Created {len(all_paragraphs)} total paragraphs")

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats['volumes_processed'] += 1

        return {
            'citations': citations_data,
            'sections': sections,
            'paragraphs': all_paragraphs
        }

    def build_documents_json(self, volumes_data: Dict[int, Dict]) -> List[Dict]:
        """
        Ø¨Ù†Ø§Ø¡ documents.json

        Args:
            volumes_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡

        Returns:
            List[Dict]: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
        """
        documents = []

        for vol_num, data in volumes_data.items():
            doc = {
                'doc_id': f'shafi_v{vol_num}',
                'type': 'document',
                'book': 'Ø§Ù„Ø´Ø§ÙÙŠ ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ù…Ø©',
                'volume': vol_num,
                'author': 'Ø§Ù„Ø´Ø±ÙŠÙ Ø§Ù„Ù…Ø±ØªØ¶Ù‰ (355-436 Ù‡Ù€)',

                # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                'stats': {
                    'total_citations': len(data['citations']['citations']),
                    'total_sections': len(data['sections']),
                    'total_paragraphs': len(data['paragraphs'])
                },

                'children_sections': [s['section_id'] for s in data['sections']]
            }

            documents.append(doc)

        return documents

    def print_final_stats(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        print(f"\n{'='*60}")
        print("ğŸ“Š Final Statistics")
        print(f"{'='*60}")

        print(f"\nğŸ¤– API Usage:")
        print(f"   Total API calls:   {self.stats['api_calls']}")
        print(f"   Input tokens:      {self.stats['total_input_tokens']:,}")
        print(f"   Output tokens:     {self.stats['total_output_tokens']:,}")
        print(f"   Total tokens:      {self.stats['total_input_tokens'] + self.stats['total_output_tokens']:,}")

        print(f"\nğŸ’° Cost:")
        print(f"   Total cost:        ${self.stats['total_cost']:.2f}")

        if self.stats['volumes_processed'] > 0:
            avg_cost = self.stats['total_cost'] / self.stats['volumes_processed']
            print(f"   Avg cost/volume:   ${avg_cost:.2f}")

        print(f"\nğŸ“š Processing:")
        print(f"   Volumes processed: {self.stats['volumes_processed']}")


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""

    print("="*60)
    print("ğŸ¯ AI-Powered Intelligent Chunking")
    print("   Multi-Level Citation-Preserving System")
    print("="*60)

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = load_config()

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    try:
        chunker = AIChunker(config)
    except ValueError as e:
        print(f"âŒ Error: {e}")
        print("\nğŸ’¡ Solution:")
        print("   1. Create .env file in project root")
        print("   2. Add: ANTHROPIC_API_KEY=sk-ant-...")
        return

    # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§
    # NOTE: Ø¶Ø¹ Ù…Ù„ÙØ§ØªÙƒ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù‡Ù†Ø§
    volumes = {
        # 1: 'data/raw/Ø§Ù„Ø´Ø§ÙÙŠ_ÙÙŠ_Ø§Ù„Ø¥Ù…Ø§Ù…Ø©_Ø¬1.txt',
        # 2: 'data/raw/Ø§Ù„Ø´Ø§ÙÙŠ_ÙÙŠ_Ø§Ù„Ø¥Ù…Ø§Ù…Ø©_Ø¬2.txt',
        # 3: 'data/raw/Ø§Ù„Ø´Ø§ÙÙŠ_ÙÙŠ_Ø§Ù„Ø¥Ù…Ø§Ù…Ø©_Ø¬3.txt',
        # 4: 'data/raw/Ø§Ù„Ø´Ø§ÙÙŠ_ÙÙŠ_Ø§Ù„Ø¥Ù…Ø§Ù…Ø©_Ø¬4.txt'
    }

    # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª
    if not volumes:
        print("\nâš ï¸  No volumes configured!")
        print("   Edit build/step2_ai_chunking.py and add your file paths")
        print("\n   Example:")
        print("   volumes = {")
        print("       1: 'data/raw/book_v1.txt',")
        print("   }")
        return

    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø¬Ø²Ø¡
    volumes_data = {}
    for vol_num, filepath in volumes.items():
        if not Path(filepath).exists():
            print(f"âš ï¸  File not found: {filepath}")
            continue

        volumes_data[vol_num] = chunker.process_volume(vol_num, filepath)

    if not volumes_data:
        print("\nâŒ No volumes were processed")
        return

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print(f"\n{'='*60}")
    print("ğŸ“¦ Building final JSON files")
    print(f"{'='*60}")

    # documents.json
    documents = chunker.build_documents_json(volumes_data)
    save_json(documents, 'data/processed/documents.json')
    print(f"âœ… Saved documents.json ({len(documents)} documents)")

    # sections.json
    all_sections = []
    for vol_data in volumes_data.values():
        all_sections.extend(vol_data['sections'])
    save_json(all_sections, 'data/processed/sections.json')
    print(f"âœ… Saved sections.json ({len(all_sections)} sections)")

    # paragraphs.json
    all_paragraphs = []
    for vol_data in volumes_data.values():
        all_paragraphs.extend(vol_data['paragraphs'])
    save_json(all_paragraphs, 'data/processed/paragraphs.json')
    print(f"âœ… Saved paragraphs.json ({len(all_paragraphs)} paragraphs)")

    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = {
        'total_documents': len(documents),
        'total_sections': len(all_sections),
        'total_paragraphs': len(all_paragraphs),
        'total_citations': sum(
            len(v['citations']['citations'])
            for v in volumes_data.values()
        ),
        'api_usage': chunker.stats
    }
    save_json(stats, 'data/processed/chunking_stats.json')
    print(f"âœ… Saved chunking_stats.json")

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    chunker.print_final_stats()

    print(f"\n{'='*60}")
    print("ğŸ‰ Chunking Complete!")
    print(f"{'='*60}")
    print(f"\nğŸ“ Output files:")
    print(f"   data/processed/documents.json")
    print(f"   data/processed/sections.json")
    print(f"   data/processed/paragraphs.json")
    print(f"   data/processed/chunking_stats.json")
    print(f"   data/processed/citations_extracted/")


if __name__ == '__main__':
    main()

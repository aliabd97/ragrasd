"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Step 4: Query Analyzer - Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†ÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ

Ø§Ù„Ù…Ù‡Ø§Ù…:
1. ÙƒØ´Ù Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
2. ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
4. ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
5. Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø«

Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 1.0.0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import re
from dataclasses import dataclass, asdict
from typing import List, Dict, Literal, Optional
from datetime import datetime
import json


@dataclass
class QueryAnalysis:
    """Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„"""

    # Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ
    original_query: str

    # Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
    language: Literal["arabic", "english", "mixed"]
    language_confidence: float  # 0-1

    # Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
    query_type: Literal[
        "factual",        # Ø³Ø¤Ø§Ù„ Ø­Ù‚ÙŠÙ‚ÙŠ: Ù…Ù†ØŸ Ù…Ø§Ø°Ø§ØŸ Ù…ØªÙ‰ØŸ
        "definition",     # ØªØ¹Ø±ÙŠÙ: Ù…Ø§ Ù‡ÙˆØŸ Ù…Ø§ Ù…Ø¹Ù†Ù‰ØŸ
        "explanation",    # Ø´Ø±Ø­: Ø§Ø´Ø±Ø­ØŒ ÙˆØ¶Ø­ØŒ ÙƒÙŠÙØŸ
        "comparison",     # Ù…Ù‚Ø§Ø±Ù†Ø©: Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†ØŒ Ù‚Ø§Ø±Ù†
        "opinion",        # Ø±Ø£ÙŠ: Ù…Ø§ Ø±Ø£ÙŠÙƒØŸ Ù‡Ù„ ØªØ¹ØªÙ‚Ø¯ØŸ
        "list",           # Ù‚Ø§Ø¦Ù…Ø©: Ø§Ø°ÙƒØ±ØŒ Ø¹Ø¯Ø¯
        "procedural"      # Ø¥Ø¬Ø±Ø§Ø¦ÙŠ: ÙƒÙŠÙ Ø£ÙØ¹Ù„ØŸ Ø®Ø·ÙˆØ§ØªØŸ
    ]
    query_type_confidence: float

    # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    keywords: List[str]

    # ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„ (Ù…Ù†ØŒ Ù…Ø§Ø°Ø§ØŒ ÙƒÙŠÙØŒ etc.)
    question_words: List[str]

    # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
    detail_level: Literal["brief", "moderate", "detailed"]

    # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
    search_strategy: Dict[str, any]

    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    metadata: Dict[str, any]

    # ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
    timestamp: str


class QueryAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠ"""

    def __init__(self):
        # Ø£Ù†Ù…Ø§Ø· ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„
        self.arabic_question_words = {
            'Ù…Ù†': 'who',
            'Ù…Ø§Ø°Ø§': 'what',
            'Ù…ØªÙ‰': 'when',
            'Ø£ÙŠÙ†': 'where',
            'ÙƒÙŠÙ': 'how',
            'Ù„Ù…Ø§Ø°Ø§': 'why',
            'Ù‡Ù„': 'yes/no',
            'Ù…Ø§': 'what',
            'Ø£ÙŠ': 'which',
            'ÙƒÙ…': 'how_many'
        }

        self.english_question_words = {
            'who', 'what', 'when', 'where', 'how', 'why',
            'which', 'whose', 'whom', 'is', 'are', 'was', 'were',
            'do', 'does', 'did', 'can', 'could', 'will', 'would'
        }

        # ÙƒÙ„Ù…Ø§Øª ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        self.type_indicators = {
            'definition': {
                'ar': ['Ù…Ø§ Ù‡Ùˆ', 'Ù…Ø§ Ù‡ÙŠ', 'ØªØ¹Ø±ÙŠÙ', 'Ù…Ø¹Ù†Ù‰', 'Ø§Ù„Ù…Ù‚ØµÙˆØ¯'],
                'en': ['what is', 'what are', 'define', 'meaning of', 'definition']
            },
            'explanation': {
                'ar': ['Ø§Ø´Ø±Ø­', 'ÙˆØ¶Ø­', 'ÙØ³Ø±', 'Ø¨ÙŠÙ†', 'ÙƒÙŠÙ'],
                'en': ['explain', 'describe', 'clarify', 'elaborate', 'how']
            },
            'comparison': {
                'ar': ['Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†', 'Ù‚Ø§Ø±Ù†', 'Ù…Ù‚Ø§Ø±Ù†Ø©', 'Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù', 'Ø§Ù„ØªØ´Ø§Ø¨Ù‡'],
                'en': ['difference between', 'compare', 'comparison', 'versus', 'vs']
            },
            'list': {
                'ar': ['Ø§Ø°ÙƒØ±', 'Ø¹Ø¯Ø¯', 'Ø£Ù…Ø«Ù„Ø©', 'Ù‚Ø§Ø¦Ù…Ø©', 'Ø£Ù†ÙˆØ§Ø¹'],
                'en': ['list', 'enumerate', 'examples', 'types of', 'mention']
            }
        }

        # ÙƒÙ„Ù…Ø§Øª ØªÙˆØ­ÙŠ Ø¨Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„
        self.detail_indicators = {
            'brief': {
                'ar': ['Ø¨Ø¥ÙŠØ¬Ø§Ø²', 'Ø¨Ø§Ø®ØªØµØ§Ø±', 'Ù…Ù„Ø®Øµ', 'Ø³Ø±ÙŠØ¹Ø§'],
                'en': ['briefly', 'summary', 'quick', 'short']
            },
            'detailed': {
                'ar': ['Ø¨Ø§Ù„ØªÙØµÙŠÙ„', 'Ù…ÙˆØ³Ø¹', 'Ø´Ø§Ù…Ù„', 'ÙƒØ§Ù…Ù„', 'Ø¬Ù…ÙŠØ¹'],
                'en': ['detailed', 'comprehensive', 'complete', 'thorough', 'all']
            }
        }

        # ÙƒÙ„Ù…Ø§Øª ØªÙˆÙ‚Ù Ø¹Ø±Ø¨ÙŠØ©
        self.arabic_stopwords = {
            'ÙÙŠ', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ù…Ù†', 'Ø¹Ù†', 'Ù…Ø¹', 'Ù‡Ø°Ø§', 'Ø°Ù„Ùƒ',
            'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ù‡Ø°Ù‡', 'ØªÙ„Ùƒ', 'Ø§Ù„', 'Ùˆ', 'Ø£Ùˆ', 'Ù„ÙƒÙ†'
        }

    def analyze(self, query: str) -> QueryAnalysis:
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„

        Args:
            query: Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡

        Returns:
            QueryAnalysis: Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„Ø©
        """
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„
        cleaned_query = self._clean_query(query)

        # ÙƒØ´Ù Ø§Ù„Ù„ØºØ©
        language, lang_confidence = self._detect_language(cleaned_query)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„
        question_words = self._extract_question_words(cleaned_query, language)

        # ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        query_type, type_confidence = self._classify_query_type(cleaned_query, language)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = self._extract_keywords(cleaned_query, language)

        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„
        detail_level = self._determine_detail_level(cleaned_query, language)

        # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø«
        search_strategy = self._build_search_strategy(
            query_type, detail_level, keywords, language
        )

        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        metadata = {
            'query_length': len(cleaned_query),
            'word_count': len(cleaned_query.split()),
            'has_question_mark': '?' in query or 'ØŸ' in query,
            'is_complex': len(keywords) > 3
        }

        return QueryAnalysis(
            original_query=query,
            language=language,
            language_confidence=lang_confidence,
            query_type=query_type,
            query_type_confidence=type_confidence,
            keywords=keywords,
            question_words=question_words,
            detail_level=detail_level,
            search_strategy=search_strategy,
            metadata=metadata,
            timestamp=datetime.now().isoformat()
        )

    def _clean_query(self, query: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙˆØ§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        query = ' '.join(query.split())
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        query = re.sub(r'([?.!ØŸ])+', r'\1', query)
        return query.strip()

    def _detect_language(self, query: str) -> tuple[str, float]:
        """
        ÙƒØ´Ù Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„

        Returns:
            (language, confidence)
        """
        # Ø¹Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', query))
        english_chars = len(re.findall(r'[a-zA-Z]', query))

        total_chars = arabic_chars + english_chars

        if total_chars == 0:
            return "arabic", 0.5  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

        arabic_ratio = arabic_chars / total_chars
        english_ratio = english_chars / total_chars

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ©
        if arabic_ratio > 0.7:
            return "arabic", arabic_ratio
        elif english_ratio > 0.7:
            return "english", english_ratio
        else:
            return "mixed", max(arabic_ratio, english_ratio)

    def _extract_question_words(self, query: str, language: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„"""
        question_words = []
        query_lower = query.lower()

        if language in ["arabic", "mixed"]:
            for ar_word in self.arabic_question_words.keys():
                if ar_word in query_lower:
                    question_words.append(ar_word)

        if language in ["english", "mixed"]:
            words = query_lower.split()
            for en_word in self.english_question_words:
                if en_word in words:
                    question_words.append(en_word)

        return list(set(question_words))

    def _classify_query_type(self, query: str, language: str) -> tuple[str, float]:
        """
        ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„

        Returns:
            (query_type, confidence)
        """
        query_lower = query.lower()
        scores = {
            'factual': 0.0,
            'definition': 0.0,
            'explanation': 0.0,
            'comparison': 0.0,
            'opinion': 0.0,
            'list': 0.0,
            'procedural': 0.0
        }

        # ÙØ­Øµ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
        for qtype, indicators in self.type_indicators.items():
            lang_key = 'ar' if language in ['arabic', 'mixed'] else 'en'
            for indicator in indicators.get(lang_key, []):
                if indicator in query_lower:
                    scores[qtype] += 1.0

        # Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        # ØªØ¹Ø±ÙŠÙ
        if any(word in query_lower for word in ['Ù…Ø§ Ù‡Ùˆ', 'Ù…Ø§ Ù‡ÙŠ', 'what is', 'what are']):
            scores['definition'] += 1.5

        # Ø´Ø±Ø­
        if any(word in query_lower for word in ['Ø§Ø´Ø±Ø­', 'ÙˆØ¶Ø­', 'explain', 'how']):
            scores['explanation'] += 1.5

        # Ù…Ù‚Ø§Ø±Ù†Ø© - Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©
        if any(word in query_lower for word in ['Ø§Ù„ÙØ±Ù‚', 'Ù‚Ø§Ø±Ù†', 'difference', 'compare']):
            scores['comparison'] += 3.0  # Ø£ÙˆÙ„ÙˆÙŠØ© Ø£Ø¹Ù„Ù‰ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©

        # Ù‚Ø§Ø¦Ù…Ø©
        if any(word in query_lower for word in ['Ø§Ø°ÙƒØ±', 'Ø¹Ø¯Ø¯', 'list', 'enumerate']):
            scores['list'] += 1.5

        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ù…Ø­Ø¯Ø¯ØŒ Ø§Ø¹ØªØ¨Ø±Ù‡ Ø³Ø¤Ø§Ù„ Ø­Ù‚ÙŠÙ‚ÙŠ
        if max(scores.values()) == 0:
            if any(word in query_lower for word in ['Ù…Ù†', 'who', 'Ù…ØªÙ‰', 'when', 'Ø£ÙŠÙ†', 'where']):
                scores['factual'] = 1.0
            else:
                scores['definition'] = 0.5  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù„Ù‰
        query_type = max(scores.keys(), key=lambda k: scores[k])
        max_score = scores[query_type]

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
        total_score = sum(scores.values())
        confidence = max_score / total_score if total_score > 0 else 0.5

        return query_type, min(confidence, 1.0)

    def _extract_keywords(self, query: str, language: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        cleaned = re.sub(r'[?.!ØŸØŒ,;:]', ' ', query)
        words = cleaned.split()

        keywords = []

        for word in words:
            word_lower = word.lower()

            # ØªØ¬Ø§Ù‡Ù„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„
            if word_lower in self.arabic_question_words:
                continue
            if word_lower in self.english_question_words:
                continue

            # ØªØ¬Ø§Ù‡Ù„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù
            if word_lower in self.arabic_stopwords:
                continue

            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
            if len(word) < 2:
                continue

            keywords.append(word)

        return keywords[:10]  # Ø£Ù‚ØµÙ‰ 10 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©

    def _determine_detail_level(self, query: str, language: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        query_lower = query.lower()

        lang_key = 'ar' if language in ['arabic', 'mixed'] else 'en'

        # ÙØ­Øµ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø²
        if any(word in query_lower for word in self.detail_indicators['brief'].get(lang_key, [])):
            return "brief"

        # ÙØ­Øµ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙØµÙŠÙ„
        if any(word in query_lower for word in self.detail_indicators['detailed'].get(lang_key, [])):
            return "detailed"

        # Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ù…ØªÙˆØ³Ø·
        return "moderate"

    def _build_search_strategy(
        self,
        query_type: str,
        detail_level: str,
        keywords: List[str],
        language: str
    ) -> Dict[str, any]:
        """Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©"""

        # Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­
        n_results_map = {
            'brief': 3,
            'moderate': 5,
            'detailed': 10
        }

        # Ø£ÙˆÙ„ÙˆÙŠØ© Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        level_priority = {
            'factual': ['paragraph', 'section', 'document'],
            'definition': ['paragraph', 'section', 'document'],
            'explanation': ['section', 'paragraph', 'document'],
            'comparison': ['section', 'document', 'paragraph'],
            'opinion': ['document', 'section', 'paragraph'],
            'list': ['section', 'paragraph', 'document'],
            'procedural': ['section', 'paragraph', 'document']
        }

        return {
            'n_results': n_results_map.get(detail_level, 5),
            'level_priority': level_priority.get(query_type, ['paragraph', 'section', 'document']),
            'use_reranking': detail_level == 'detailed',
            'expand_query': len(keywords) < 3,
            'language': language,
            'search_modes': self._suggest_search_modes(query_type)
        }

    def _suggest_search_modes(self, query_type: str) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©"""
        modes = ['semantic']  # Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ

        # Ø£Ù†ÙˆØ§Ø¹ Ù…Ø¹ÙŠÙ†Ø© ØªØ³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        if query_type in ['factual', 'list']:
            modes.append('keyword')

        # Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ Ù‚Ø¯ Ù†Ø­ØªØ§Ø¬ Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù…
        if query_type == 'comparison':
            modes.append('multi_query')

        return modes

    def print_analysis(self, analysis: QueryAnalysis, verbose: bool = False):
        """Ø·Ø¨Ø§Ø¹Ø© Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø³Ù‚"""
        print("\n" + "="*70)
        print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„")
        print("="*70)

        print(f"\nğŸ“ Ø§Ù„Ø³Ø¤Ø§Ù„: {analysis.original_query}")
        print(f"\nğŸŒ Ø§Ù„Ù„ØºØ©: {analysis.language} ({analysis.language_confidence:.0%})")
        print(f"ğŸ“‹ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„: {analysis.query_type} ({analysis.query_type_confidence:.0%})")
        print(f"ğŸ“ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØµÙŠÙ„: {analysis.detail_level}")

        if analysis.question_words:
            print(f"\nâ“ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„: {', '.join(analysis.question_words)}")

        if analysis.keywords:
            print(f"\nğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©:")
            for i, kw in enumerate(analysis.keywords, 1):
                print(f"   {i}. {kw}")

        print(f"\nğŸ¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø«:")
        print(f"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {analysis.search_strategy['n_results']}")
        print(f"   â€¢ Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª: {' â†’ '.join(analysis.search_strategy['level_priority'])}")
        print(f"   â€¢ Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ø¨Ø­Ø«: {', '.join(analysis.search_strategy['search_modes'])}")

        if verbose:
            print(f"\nğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:")
            for key, value in analysis.metadata.items():
                print(f"   â€¢ {key}: {value}")

        print("\n" + "="*70)

    def to_json(self, analysis: QueryAnalysis) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¥Ù„Ù‰ JSON"""
        return json.dumps(asdict(analysis), ensure_ascii=False, indent=2)


def main():
    """Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… QueryAnalyzer"""

    analyzer = QueryAnalyzer()

    # Ø£Ù…Ø«Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©
    test_queries = [
        "Ù…Ù† Ù‡Ùˆ Ø§Ù„Ø´Ø±ÙŠÙ Ø§Ù„Ù…Ø±ØªØ¶Ù‰ØŸ",
        "Ù…Ø§ Ù‡Ùˆ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¥Ù…Ø§Ù…Ø© ÙÙŠ Ø§Ù„ÙÙƒØ± Ø§Ù„Ø´ÙŠØ¹ÙŠØŸ",
        "Ø§Ø´Ø±Ø­ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¹ØµÙ…Ø©",
        "Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ù…Ø§Ù…Ø© ÙˆØ§Ù„Ø®Ù„Ø§ÙØ©ØŸ",
        "Ø§Ø°ÙƒØ± Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø¯Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù…Ø§Ù…Ø©",
        "What is Imamah in Islamic theology?",
        "ÙƒÙŠÙ ÙŠÙØ«Ø¨Øª ÙˆØ¬ÙˆØ¨ Ø§Ù„Ø¥Ù…Ø§Ù…Ø©ØŸ"
    ]

    print("\n" + "="*70)
    print("ğŸš€ Step 4: Query Analyzer - Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©")
    print("="*70)

    for i, query in enumerate(test_queries, 1):
        print(f"\n\n{'â”€'*70}")
        print(f"Ø§Ø®ØªØ¨Ø§Ø± {i}/{len(test_queries)}")
        print(f"{'â”€'*70}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„
        analysis = analyzer.analyze(query)

        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
        analyzer.print_analysis(analysis)

    print("\n\n" + "="*70)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!")
    print("="*70)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-Level Chunking Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯ÙŠÙ†ÙŠØ©
======================================
ØªÙ‚Ø·ÙŠØ¹ Ù‡Ø±Ù…ÙŠ: Document â†’ Section â†’ Paragraph
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RAW_DIR = Path("/mnt/user-data/uploads")
OUTPUT_DIR = Path("/mnt/user-data/outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_pages(text: str) -> List[Dict]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
    pages = []
    
    # Ù†Ù…Ø· 1: Ø§Ù„ØµÙØ­Ø© X
    # Ù†Ù…Ø· 2: [Ø§Ù„ØµÙØ­Ø© X Ù…Ù† Y]
    pattern = r'(?:^|\n)(?:\[)?Ø§Ù„ØµÙØ­Ø©\s+(\d+)'
    
    matches = list(re.finditer(pattern, text, re.MULTILINE))
    
    for i, match in enumerate(matches):
        page_num = int(match.group(1))
        start = match.end()
        
        # Ù†Ù‡Ø§ÙŠØ© = Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø£Ùˆ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù†Øµ
        end = matches[i+1].start() if i+1 < len(matches) else len(text)
        
        page_text = text[start:end].strip()
        
        if page_text:
            pages.append({
                "page_num": page_num,
                "text": page_text
            })
    
    return pages


def count_words(text: str) -> int:
    """Ø¹Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    return len(text.split())


def count_citations(text: str) -> int:
    """Ø¹Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
    pattern = r'\((\d+)\)'
    return len(re.findall(pattern, text))


def extract_citation_refs(text: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
    pattern = r'\((\d+)\)'
    return re.findall(pattern, text)


def generate_title(text: str, max_len: int = 60) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ø§Ù„Ù†Øµ"""
    # Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø©
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    if not lines:
        return "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
    
    first = lines[0]
    # ØªÙ†Ø¸ÙŠÙ
    first = re.sub(r'\(\d+\)', '', first)
    first = first.strip()
    
    if len(first) > max_len:
        return first[:max_len] + "..."
    return first


def classify_content(text: str) -> str:
    """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
    text_lower = text[:500].lower()
    
    if any(x in text_lower for x in ['Ù…Ù‚Ø¯Ù…Ø©', 'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡', 'Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡']):
        return "Ù…Ù‚Ø¯Ù…Ø©"
    elif any(x in text_lower for x in ['Ù‚Ø§Ù„', 'ÙÙ‚Ø§Ù„Øª', 'Ù‚Ø§Ù„ÙˆØ§']):
        return "Ø¹Ø±Ø¶ Ø¢Ø±Ø§Ø¡"
    elif any(x in text_lower for x in ['Ø¯Ù„ÙŠÙ„', 'Ø¨Ø±Ù‡Ø§Ù†', 'Ø§Ù„Ø­Ø¬Ø©']):
        return "Ø£Ø¯Ù„Ø©"
    elif any(x in text_lower for x in ['Ø§Ù„Ø¬ÙˆØ§Ø¨', 'ÙÙ†Ù‚ÙˆÙ„', 'ÙˆØ§Ù„Ø±Ø¯']):
        return "Ø±Ø¯ÙˆØ¯"
    elif any(x in text_lower for x in ['Ø®Ù„Ø§ØµØ©', 'Ø§Ù„Ø®Ù„Ø§ØµØ©', 'ÙØªØ¨ÙŠÙ†']):
        return "Ø®Ù„Ø§ØµØ©"
    else:
        return "Ù†Øµ Ø¹Ø§Ù…"


def is_good_break(para: str) -> bool:
    """Ù‡Ù„ Ù†Ù‚Ø·Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù‚Ø·Ø¹ØŸ"""
    para = para.strip()
    
    # Ù†Ù‡Ø§ÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ©
    if para.endswith(('.', 'ØŸ', '!')):
        # Ù„ÙŠØ³ Ø¨Ø¹Ø¯ Ù…ØµØ¯Ø± Ù…Ø¨Ø§Ø´Ø±Ø©
        if not re.search(r'\(\d+\)$', para):
            return True
    
    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 1: Documents
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_documents(volumes: List[Tuple[int, str, Dict]]) -> List[Dict]:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Documents
    ÙƒÙ„ Ø¬Ø²Ø¡ = document ÙˆØ§Ø­Ø¯
    """
    documents = []
    
    for vol_num, vol_text, citations_data in volumes:
        pages = extract_pages(vol_text)
        
        # Ù…Ù„Ø®Øµ Ø¨Ø³ÙŠØ· (Ø£ÙˆÙ„ 500 ÙƒÙ„Ù…Ø©)
        first_text = ' '.join([p['text'] for p in pages[:3]])
        summary_words = first_text.split()[:500]
        summary = ' '.join(summary_words) + "..."
        
        doc = {
            "doc_id": f"shafi_v{vol_num}",
            "type": "document",
            "book": "Ø§Ù„Ø´Ø§ÙÙŠ ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ù…Ø©",
            "volume": vol_num,
            "author": "Ø§Ù„Ø´Ø±ÙŠÙ Ø§Ù„Ù…Ø±ØªØ¶Ù‰ (355-436 Ù‡Ù€)",
            
            "summary": summary,
            
            "stats": {
                "pages": len(pages),
                "words": count_words(vol_text),
                "citations": citations_data.get('total_citations', 0)
            },
            
            "children_sections": []
        }
        
        documents.append(doc)
        
        print(f"âœ… Document {doc['doc_id']}: {doc['stats']['pages']} ØµÙØ­Ø©")
    
    return documents


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 2: Sections
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_sections(doc_id: str, pages: List[Dict], 
                   citations: List[Dict], 
                   pages_per_section: int = 4) -> List[Dict]:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Sections
    ÙƒÙ„ 4 ØµÙØ­Ø§Øª = section
    """
    sections = []
    section_num = 1
    
    for i in range(0, len(pages), pages_per_section):
        section_pages = pages[i:i+pages_per_section]
        
        # Ø¯Ù…Ø¬ Ø¢Ø®Ø± section ØµØºÙŠØ± Ù…Ø¹ Ø§Ù„Ø³Ø§Ø¨Ù‚
        if len(section_pages) < 2 and sections:
            last_sec = sections[-1]
            last_sec['pages'].extend([p['page_num'] for p in section_pages])
            last_sec['text'] += "\n\n" + "\n\n".join([p['text'] for p in section_pages])
            last_sec['stats']['word_count'] = count_words(last_sec['text'])
            continue
        
        section_text = "\n\n".join([p['text'] for p in section_pages])
        page_nums = [p['page_num'] for p in section_pages]
        
        # Ø¹Ù†ÙˆØ§Ù†
        title = generate_title(section_text)
        
        # Ù…ØµØ§Ø¯Ø± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…
        cite_refs = extract_citation_refs(section_text)
        section_citations = [
            c for c in citations 
            if c.get('number') in cite_refs
        ]
        
        section = {
            "section_id": f"{doc_id}_sec_{section_num:03d}",
            "type": "section",
            "parent_doc": doc_id,
            
            "title": title,
            "pages": page_nums,
            "text": section_text,
            
            "content_type": classify_content(section_text),
            
            "stats": {
                "word_count": count_words(section_text),
                "citation_count": len(section_citations)
            },
            
            "citations": section_citations[:10],  # Ø£ÙˆÙ„ 10 ÙÙ‚Ø·
            
            "children_paragraphs": [],
            
            "next_section": None,
            "prev_section": None
        }
        
        sections.append(section)
        section_num += 1
    
    # Ø±ÙˆØ§Ø¨Ø· next/prev
    for i, sec in enumerate(sections):
        if i > 0:
            sec['prev_section'] = sections[i-1]['section_id']
        if i < len(sections) - 1:
            sec['next_section'] = sections[i+1]['section_id']
    
    return sections


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 3: Paragraphs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_paragraphs(section: Dict, citations: List[Dict],
                     min_words: int = 800, 
                     max_words: int = 1200) -> List[Dict]:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Paragraphs
    ØªÙ‚Ø³ÙŠÙ… Ø°ÙƒÙŠ: 800-1200 ÙƒÙ„Ù…Ø©
    """
    section_text = section['text']
    section_id = section['section_id']
    parent_doc = section['parent_doc']
    
    paragraphs = []
    para_num = 1
    
    # ØªÙ‚Ø³ÙŠÙ… Ø£ÙˆÙ„ÙŠ Ø¨Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
    natural_paras = section_text.split('\n\n')
    
    current_text = ""
    
    for nat_para in natural_paras:
        nat_para = nat_para.strip()
        if not nat_para:
            continue
        
        current_text += nat_para + "\n\n"
        word_count = count_words(current_text)
        
        should_end = False
        
        if word_count >= max_words:
            should_end = True
        elif word_count >= min_words and is_good_break(nat_para):
            should_end = True
        
        if should_end:
            # Ø¥Ù†Ø´Ø§Ø¡ paragraph
            cite_refs = extract_citation_refs(current_text)
            para_citations = [
                c for c in citations 
                if c.get('number') in cite_refs
            ]
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© (Ø£ÙˆÙ„ Ø±Ù‚Ù… ÙÙŠ Ø§Ù„Ù†Øµ)
            page_match = re.search(r'Ø§Ù„ØµÙØ­Ø© (\d+)', current_text)
            page_num = int(page_match.group(1)) if page_match else section['pages'][0]
            
            paragraph = {
                "para_id": f"{section_id}_para_{para_num:03d}",
                "type": "paragraph",
                "parent_section": section_id,
                "parent_doc": parent_doc,
                
                "text": current_text.strip(),
                
                "stats": {
                    "word_count": word_count,
                    "page": page_num,
                    "citation_count": len(para_citations)
                },
                
                "content_type": classify_content(current_text),
                "citations": para_citations[:5],  # Ø£ÙˆÙ„ 5
                
                "next_para": None,
                "prev_para": None
            }
            
            paragraphs.append(paragraph)
            
            current_text = ""
            para_num += 1
    
    # Ø§Ù„Ø¨Ù‚ÙŠØ©
    if current_text.strip():
        cite_refs = extract_citation_refs(current_text)
        para_citations = [
            c for c in citations 
            if c.get('number') in cite_refs
        ]
        
        page_match = re.search(r'Ø§Ù„ØµÙØ­Ø© (\d+)', current_text)
        page_num = int(page_match.group(1)) if page_match else section['pages'][0]
        
        paragraph = {
            "para_id": f"{section_id}_para_{para_num:03d}",
            "type": "paragraph",
            "parent_section": section_id,
            "parent_doc": parent_doc,
            
            "text": current_text.strip(),
            
            "stats": {
                "word_count": count_words(current_text),
                "page": page_num,
                "citation_count": len(para_citations)
            },
            
            "content_type": classify_content(current_text),
            "citations": para_citations[:5],
            
            "next_para": None,
            "prev_para": None
        }
        
        paragraphs.append(paragraph)
    
    # Ø±ÙˆØ§Ø¨Ø· next/prev
    for i, para in enumerate(paragraphs):
        if i > 0:
            para['prev_para'] = paragraphs[i-1]['para_id']
        if i < len(paragraphs) - 1:
            para['next_para'] = paragraphs[i+1]['para_id']
    
    return paragraphs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main Processing
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("="*70)
    print("ğŸ¯ Multi-Level Chunking")
    print("="*70)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("\nğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    
    volumes = []
    all_citations = {}
    
    for vol_num in range(1, 5):
        txt_file = RAW_DIR / f"Ø§Ù„Ø´Ø§ÙÙŠ_ÙÙŠ_Ø§Ù„Ø¥Ù…Ø§Ù…Ø©_Ø¬{vol_num}.txt"
        cite_file = RAW_DIR / f"citations_Ø¬{vol_num}.json"
        
        print(f"  - Ø§Ù„Ø¬Ø²Ø¡ {vol_num}...", end=" ")
        
        with open(txt_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        with open(cite_file, 'r', encoding='utf-8') as f:
            citations = json.load(f)
        
        volumes.append((vol_num, text, citations))
        all_citations[vol_num] = citations.get('citations', [])
        
        print("âœ…")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 1: Documents
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\nğŸ“š Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 1: Documents...")
    documents = create_documents(volumes)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 2 & 3: Sections & Paragraphs
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\nğŸ“‘ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 2-3: Sections & Paragraphs...")
    
    all_sections = []
    all_paragraphs = []
    
    for vol_num, vol_text, citations_data in volumes:
        doc_id = f"shafi_v{vol_num}"
        print(f"\n  ğŸ“– Ù…Ø¹Ø§Ù„Ø¬Ø© {doc_id}...")
        
        pages = extract_pages(vol_text)
        citations = all_citations[vol_num]
        
        # Sections
        sections = create_sections(doc_id, pages, citations)
        print(f"    âœ… {len(sections)} sections")
        
        # Paragraphs
        section_para_count = 0
        for section in sections:
            paras = create_paragraphs(section, citations)
            section['children_paragraphs'] = [p['para_id'] for p in paras]
            all_paragraphs.extend(paras)
            section_para_count += len(paras)
        
        print(f"    âœ… {section_para_count} paragraphs")
        
        # ØªØ­Ø¯ÙŠØ« document
        doc = next(d for d in documents if d['doc_id'] == doc_id)
        doc['children_sections'] = [s['section_id'] for s in sections]
        doc['stats']['sections'] = len(sections)
        doc['stats']['paragraphs'] = section_para_count
        
        all_sections.extend(sections)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Structure
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\nğŸ—ºï¸ Ø¨Ù†Ø§Ø¡ Structure...")
    
    structure = {
        "book": "Ø§Ù„Ø´Ø§ÙÙŠ ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ù…Ø©",
        "author": "Ø§Ù„Ø´Ø±ÙŠÙ Ø§Ù„Ù…Ø±ØªØ¶Ù‰",
        "total_volumes": 4,
        "total_documents": len(documents),
        "total_sections": len(all_sections),
        "total_paragraphs": len(all_paragraphs),
        
        "total_pages": sum(d['stats']['pages'] for d in documents),
        "total_words": sum(d['stats']['words'] for d in documents),
        "total_citations": sum(d['stats']['citations'] for d in documents),
        
        "hierarchy": {
            "documents": [d['doc_id'] for d in documents],
            "sections_per_document": {
                d['doc_id']: d['stats']['sections'] 
                for d in documents
            },
            "paragraphs_per_document": {
                d['doc_id']: d['stats']['paragraphs'] 
                for d in documents
            }
        },
        
        "created_at": datetime.now().isoformat()
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\nğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª...")
    
    files = {
        'structure.json': structure,
        'documents.json': documents,
        'sections.json': all_sections,
        'paragraphs.json': all_paragraphs
    }
    
    for filename, data in files.items():
        filepath = OUTPUT_DIR / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        size_kb = filepath.stat().st_size / 1024
        print(f"  âœ… {filename}: {size_kb:.1f} KB")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\n" + "="*70)
    print("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
    print("="*70)
    print(f"ğŸ“š Documents: {len(documents)}")
    print(f"ğŸ“‘ Sections: {len(all_sections)}")
    print(f"ğŸ“ Paragraphs: {len(all_paragraphs)}")
    print(f"ğŸ“„ Pages: {structure['total_pages']}")
    print(f"ğŸ“– Words: {structure['total_words']:,}")
    print(f"ğŸ”— Citations: {structure['total_citations']}")
    print("="*70)
    print("âœ¨ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­!")
    print("="*70)


if __name__ == "__main__":
    main()
